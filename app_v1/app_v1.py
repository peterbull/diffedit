# AUTOGENERATED! DO NOT EDIT! File to edit: ../diffedit.ipynb.

# %% auto 0
__all__ = ['tokenizer', 'text_encoder', 'vae', 'unet', 'q', 'r', 'height', 'width', 'num_inference_steps', 'guidance_scale',
           'batch_size']

# %% ../diffedit.ipynb 5
import torch
from transformers import CLIPTextModel, CLIPTokenizer
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from PIL import Image

import logging

logging.disable(logging.WARNING)
torch.manual_seed(1);

# %% ../diffedit.ipynb 7
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14", torch_dtype=torch.float16).to("cuda")

# %% ../diffedit.ipynb 9
from diffusers import AutoencoderKL, UNet2DConditionModel

# %% ../diffedit.ipynb 10
vae = AutoencoderKL.from_pretrained("stabilityai/sd-vae-ft-ema", torch_dtype=torch.float16).to("cuda")
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet", torch_dtype=torch.float16).to("cuda")

# %% ../diffedit.ipynb 12
from diffusers import LMSDiscreteScheduler

# %% ../diffedit.ipynb 15
q = ["dog"]
r = ["cat"]

height = 512
width = 512
num_inference_steps = 70
guidance_scale = 7.5
batch_size = 1
