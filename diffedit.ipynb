{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7cd00f-3fdf-4f9b-bef4-714c8c1a3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp app_v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e59082ab-83f8-466a-b051-855abc3f7c20",
   "metadata": {},
   "source": [
    "## Reimplementing DiffEdit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13fca58c-3566-4e1d-b893-a6b589b01453",
   "metadata": {},
   "source": [
    "In this notebook we're going to reimplement the semantic image editing process illustrated in the [DiffEdit](https://arxiv.org/abs/2210.11427) paper. In the paper, the authors proposed using text input to create a mask of the queried object, and essentially using an img2img type of processing, such that changes could be made to the object without making changes to the context of the image. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c05b16e0-7bf2-4884-8e24-2a7c9119ddef",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27b316db-a812-4d49-821f-07350c6445b5",
   "metadata": {},
   "source": [
    "<center><img alt=\"DiffEdit Workflow\" width=\"1000\" src=\"imgs/diffusion_method2.jpg\" /></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fdb7ffc-7fdc-4601-a242-3b19772a5538",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbeebe55-bed1-421b-9212-af89b2cb8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from base64 import b64encode\n",
    "\n",
    "import os\n",
    "import numpy\n",
    "import torch\n",
    "from diffusers import AutoencoderKL, LMSDiscreteScheduler, UNet2DConditionModel\n",
    "from huggingface_hub import notebook_login\n",
    "from fastai.vision.all import *\n",
    "\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms as tfms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "import tsensor\n",
    "from lolviz import *\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
    "logging.disable(logging.WARNING)\n",
    "torch.manual_seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b79b142-3fe5-474d-9092-73d218b8203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6a205b5-8df7-449c-ab65-5181bdde73c9",
   "metadata": {},
   "source": [
    "## Load Autoencoder, VAE, ClipTokenizer, Clip Text Encoder, and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e580dbea-ccd1-451b-addb-ed66c161bb87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoencoderKL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vae \u001b[39m=\u001b[39m AutoencoderKL\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mCompVis/stable-diffusion-v1-4\u001b[39m\u001b[39m\"\u001b[39m, subfolder\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvae\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m CLIPTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mopenai/clip-vit-large-patch14\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m text_encoder \u001b[39m=\u001b[39m CLIPTextModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mopenai/clip-vit-large-patch14\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoencoderKL' is not defined"
     ]
    }
   ],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "\n",
    "vae = vae.to(torch_device).half()\n",
    "text_encoder = text_encoder.to(torch_device).half()\n",
    "unet = unet.to(torch_device).half();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f63753c-ffe4-4077-985c-7ead9e4bbf28",
   "metadata": {},
   "source": [
    "## Define Functions for Imamge -> Latent and Latent -> Image Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e54cf81-704d-46c0-836e-e6ba2e631d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_latent(input_im):\n",
    "    # Single image -> single latent in a batch, size (1, 4, 64, 64)\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device).half()*2-1)\n",
    "    return 0.18215 * latent.latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a908f43d-f80f-4f51-bc7f-efe9025df5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_to_pil(latents):\n",
    "    # batch of latents -> list of images\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b53caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_embedding(prompt):\n",
    "    max_length = tokenizer.model_max_length\n",
    "    tokens = tokenizer(prompt, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(tokens.input_ids.to(torch_device))[0]\n",
    "    return text_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9e13101-e0ac-4b0e-9efb-9e09bdb11427",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf142b4d-fc7e-409e-9028-ab89b4293185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "im = Image.open('imgs/IMG_4104_512.jpg')\n",
    "im"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a47efcd7-30e9-4e24-87cc-3fc3c1d43c4d",
   "metadata": {},
   "source": [
    "## Set up the Scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ced7746b-643a-410b-b916-c7c133088f05",
   "metadata": {},
   "source": [
    "Set the number of sampling steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "628ba580-a256-40af-a60f-d1b5754f31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a wolf staring at the viewer, by Howard Arkley\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abce64c8",
   "metadata": {},
   "source": [
    "Convert the prompt to a text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050e3343",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text \u001b[39m=\u001b[39m text_encoder(prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "text = text_encoder(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc09b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9a38876",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b08d6e6d",
   "metadata": {},
   "source": [
    "Prep Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a953272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds = get_prompt_embedding(prompt)\n",
    "uncond_embeds = get_prompt_embedding('')\n",
    "text_embeds = torch.cat([uncond_embeds, prompt_embeds])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c69b644",
   "metadata": {},
   "source": [
    "Prep Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb5804f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(num_inference_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = scheduler.config.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df2d49e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_timesteps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_timesteps(scheduler, num_inference_steps, \u001b[39m0.5\u001b[39m, torch_device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_timesteps' is not defined"
     ]
    }
   ],
   "source": [
    "get_timesteps(scheduler, num_inference_steps, 0.5, torch_device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7243a4c",
   "metadata": {},
   "source": [
    "Prep Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f372677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b50b301-3511-40dc-a598-dbf8cd4f5524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export successful\n"
     ]
    }
   ],
   "source": [
    "import nbdev\n",
    "nbdev.export.nb_export('diffedit.ipynb', 'app_v1')\n",
    "print(\"export successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f8004-41a0-423b-8177-61e0ee660d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
